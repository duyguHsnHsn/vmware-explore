{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee62f01",
   "metadata": {},
   "source": [
    "# Welcome to the Versatile Data Kit Demo Example!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda79260",
   "metadata": {},
   "source": [
    "## Workshop Steps\n",
    "Now that you have opened up the MyBinder environment and are reading this, you are already on the right track! Inside this environment,\n",
    "you will also find:\n",
    "* samples: This is a folder containing the base of the scripts that you will be working with to finish the exercise. Please look for the triple exclamation points (!!!) as that means that you are being asked to write some code to get things to work!\n",
    "* README.md: This is just the README file you saw on the Github page.\n",
    "* requirements.txt: This is a list of the required libraries that were installed upon startup.\n",
    "* exercise.ipynb: The file you are reading right now! Think of this as your home page.\n",
    "* Other system files - postBuild and start: No need to worry about these. They are needed for the setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a7011",
   "metadata": {},
   "source": [
    "### Step 0: Explore VDK's Functionalities\n",
    "A simple command like that found in the exercise.ipynb \"!vdk --help\" gives you all the information you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfd927",
   "metadata": {},
   "source": [
    "### Step 1: Identify the business process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89e488",
   "metadata": {},
   "source": [
    "**Business request**: \n",
    "\n",
    "We work for Volkswagen. Latest market research has shown that our customers need to understand better their battery usage in order to be able to plan their trips better. We want to create application that will allow them to predict battery drainage based on different environment parameters (speed of travel, are they using heated seats, etc.). Using the app our customers would be able to plan their trips much better (e.g they'd drive slowly or they'd not use the heated seats, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7622e41",
   "metadata": {},
   "source": [
    "**What is the business process we want to model ?**\n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> ...\n",
    "\n",
    "How do we find the business process \n",
    "\n",
    "* Look from the customer or user persona's point of view.\n",
    "* It is usally expressed as action verbs/events. \n",
    "* It generates key performance indicators\n",
    "* Often it might be supported by an operational system\n",
    "* Communication: talk to product team, marketing team, customer excelence team and the customers themselves (if feasible).\n",
    "\n",
    "\n",
    "**Task: Describe the business process for the business reuqest we are tracking. \n",
    "        In our case we will make it up. In reality *never* get tempted to make things up. It must be a real business process.**\n",
    "\n",
    "\n",
    "<!-- The business process we need to model is a car owner taking a trip using VW electric cars we'll track the EV consumption.. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17356a",
   "metadata": {},
   "source": [
    "#### Ingest data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b7242-8f5a-4358-a5b7-8b2dda90aeab",
   "metadata": {},
   "source": [
    "We are using the data provided by Göktuğ Özgül on Kaggle.com\n",
    "Now let's ingest our data.  \n",
    "\n",
    "We are going to an easy way to ingest CSV data using Versatile Data Kit: `vdk ingest-csv`. <br>\n",
    "Make sure to check the help! It has pretty good documentation and examples. \n",
    "Full tutorial can also been seen [here](https://github.com/vmware/versatile-data-kit/wiki/Ingesting-local-CSV-file-into-Database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542bdd7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! vdk ingest-csv --help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546670dc",
   "metadata": {},
   "source": [
    "The command did not work :) . That's because we need to install a plugin. Versatile Data Kit is extremely pluggbale and versatile. \n",
    "\n",
    "Let's install vdk-csv plugin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae651c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vdk-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87941f16",
   "metadata": {},
   "source": [
    "For this excercise we are going to use Trino. Let's tell VDK that we want to ingest and query data using Trino (the connection settingis preconfigured for us.). \n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> ...\n",
    "\n",
    "Change VDK_TRINO_SCHEMA with one of your own for example \"aivanov\" (for Antoni Ivanov) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7cd219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%env VDK_DB_DEFAULT_TYPE=trino\n",
    "%env VDK_INGEST_METHOD_DEFAULT=trino\n",
    "%env VDK_TRINO_SCHEMA=ai4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk trino-query -q \"create schema if not exists $VDK_TRINO_SCHEMA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb16098",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk trino-query -q \"create table if not exists staging_area_trips (\\\n",
    "trip                       INTEGER ,\\\n",
    "date                       VARCHAR ,\\\n",
    "location                   VARCHAR ,\\\n",
    "tyres                      VARCHAR ,\\\n",
    "temperature_start_c        REAL ,\\\n",
    "temperature_end_c          REAL ,\\\n",
    "distance_km                INTEGER ,\\\n",
    "duration_minutes           INTEGER ,\\\n",
    "average_speed_kmh          INTEGER ,\\\n",
    "average_consumption_kwhkm  REAL ,\\\n",
    "charge_level_start         REAL ,\\\n",
    "charge_level_end           REAL ,\\\n",
    "ac_c                       VARCHAR ,\\\n",
    "heated_front_seats_level   INTEGER ,\\\n",
    "mode                       VARCHAR \\\n",
    " )\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059d126-70b5-4fba-9cd3-5a022ca0f2c8",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! vdk ingest-csv -f \"VW ID. 3 Pro Max EV Consumption.csv\"  --table-name \"staging_area_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dee313",
   "metadata": {},
   "source": [
    "To verify that the data is ingested as expected, let's query the database. \n",
    "\n",
    "VDK comes with a easy way to query any pre-configured database using CLI: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdk trino-query -q \"SELECT * FROM staging_area_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5615dc",
   "metadata": {},
   "source": [
    "<font color='blue'>**NOTICE:**</font> `vdk ingest-csv` might be great if you want to ingest a signle file quickly. But if we need continious ingestion over time of a lot more data we'd use Ingestion Data Jobs. For more information see [Ingestion examples](https://github.com/vmware/versatile-data-kit/wiki/Examples#ingestion-examples) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a05cb",
   "metadata": {},
   "source": [
    "### Step 2: Identify the Grain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625dba7f",
   "metadata": {},
   "source": [
    "At this point, we know the questions that we want to answer, we understand the business process and it’s now time to start prepping up our data warehouse design.\n",
    "\n",
    "To be able to answer the request that we outlined above, we know that we will probably need the following attributes in our model.\n",
    "\n",
    "* consumption per trip, duration for that trip (both in time and distance) , status \n",
    "\n",
    "And following descriptive data: \n",
    "\n",
    "* vehicle information - model, battery type, etc. \n",
    "\n",
    "\n",
    "Reminder: The grain is the business definition of what a single fact table record represents.  Thehe grain is the description of the measurement event in the physical world that gives rise to a measurement. Kimball proposes three types: Transactional, Periodic snapshot and Accumulating snapshot.\n",
    "\n",
    "For example: When the grocery store scanner measures the quantity and the charged price of a product being purchased, the grain is literally the beep of the scanner. That is a great grain definition! \n",
    "\n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> ...\n",
    "\n",
    "**TASK: What is our grain definition for the above business request?**\n",
    "\n",
    "<!-- \n",
    "In our case we probably can work with more aggregated data - like daily snapshot or weekly . But for the purposes of this tutorial we'd proceed with transactional data. The grain would be the end of the a full trip. \n",
    "\n",
    "So we seem we'd need a bit of  transactional data, (a single trip can be thought of as a transaction), \n",
    "We need also  descriptive data that goes along with these transactions (currently car temperature features but in the future all sorts of information that may impact battery life). This is a good indication that we may need both a fact and dimension.\n",
    "\n",
    "To remind again, we can think of a fact table as a place where we can store measurements and a dimension table where we store descriptive attributes associated with the facts measurement.\n",
    "\n",
    "**The grain of our data is at the transaction level** for a trip (we are going to track each trip). Possibly some time of aggregated level would do as well (snapshot grain). But this would give more flexibility to our ML model.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea47ca4c",
   "metadata": {},
   "source": [
    "### Step 3: Identify the dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f70291c",
   "metadata": {},
   "source": [
    "  Once the grain has been properly declared, the dimensions typically can easily be identified as they represent the “who, what, where, when, why, and how” associated with the event. A robust set of dimensions representing all possible descriptions should be identified. In our case we'd need data about the vehicle information (manufacturer, model, battery type etc.) \n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> ...\n",
    "\n",
    "**TASK: What dimensions are relevant to the above business request?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7a66ca",
   "metadata": {},
   "source": [
    "### Step 4: Identify the facts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad88e5",
   "metadata": {},
   "source": [
    "Facts are determined by answering the question, “What is the process measuring? In other words the facts are the metrics that business users are concerned about. These must be appropriately defined in accordance with the declared grain. If not, they should be placed in a different fact table. Usually, facts are numerical data, such as total cost or order quantity. \n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> ...\n",
    "\n",
    "**TASK: What is are fact table(s) relevant to the above business request?**\n",
    "\n",
    "<!--\n",
    "\n",
    "We have identified that our main fact table is \"fact_trips\" at transaction grain. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73a553",
   "metadata": {},
   "source": [
    "### Data transformation: Create a Data Job\n",
    "Now that we have explored VDK's capabilities, and followed the dimensiton business process, let's create our transformation data jobs. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b142837f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!vdk create -n build-dimensional-model -t team-awesome -p /home/jovyan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a57e2d",
   "metadata": {},
   "source": [
    "When you create a data job, VDK automatically downloads some template scripts and files, so that you can get accustomed to the data job's structure. They are super helpful in getting you ready to run your own data jobs. However, let's go ahead and delete these for our example, since we won't be starting from scratch, but please check them out! Alternatively, you can explore the 'vdk create --no-template' option, if you do not want these templates downloaded. Let's go ahead and delete the following files:\n",
    "* The SQL script: our example does not do anything with SQL.\n",
    "* The sample Python script: we already have moved four sample Python scripts, so we won't be needing this.\n",
    "* README.md: We already have a README for the entire example, so we can get rid of this.\n",
    "* requirements.txt: Each data job would need this file if the data job relies on external libraries that VDK does not have. In our case, MyBinder installed those upon startup, so we won't be needing this either.\n",
    "\n",
    "As such, please run the code below to delete them:\n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> Please change 'build-dimensional-model' to the name of your data job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4051888",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm \"build-dimensional-model/10_sql_step.sql\"\n",
    "! rm \"build-dimensional-model/20_python_step.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95527b59",
   "metadata": {},
   "source": [
    "Great! Now you're all set up with the data job:\n",
    "* You have created a data job.\n",
    "* You have deleted the template files that you do not need.\n",
    "* You have moved the sample scripts we provided to the data job sub-folder.\n",
    "* You have moved the raw CSV file to the data job sub-folder for a neater environment!\n",
    "\n",
    "The next step is to begin working on each script in the data job! Let's do it!\n",
    "\n",
    "We have 2 files (we can see them in samples/build-dimensional-model). Feel free to copy it.\n",
    "\n",
    "* 10_fact_trips_create_table.sql - we define the schema of our fact table \n",
    "* 20_fact_trips_update.py - this will load the data into our fact table. \n",
    "\n",
    "The code looks something like this \n",
    "```\n",
    "job_input.execute_template(\n",
    "        template_name='periodic_snapshot',\n",
    "        template_args={\n",
    "            'source_view': 'staging_area_trips',\n",
    "            'target_table': 'fact_trips',\n",
    "            'last_arrival_ts': 'date'\n",
    "        },\n",
    "    )\n",
    "```\n",
    "\n",
    "We are using period_snapshot (called also append). <br>\n",
    "Append strategy appends a snapshot of records observed between time t1 and t2 from the source table to the target table, truncating all present target table records observed after t1. <br>\n",
    "The strategy can be used for updating Periodic Snapshot Fact Tables or transaction fact table in data warehousing ETL jobs. <br>\n",
    "It suitable to update records where late arrival data is expected. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk run samples/build-dimensional-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4079981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdk trino-query -q \"select * from fact_trips\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e3d0c",
   "metadata": {},
   "source": [
    "<font color='blue'>**NOTICE:**</font> <br>\n",
    "For other type of Kimball templates you can checkout [example documentation](https://github.com/vmware/versatile-data-kit/wiki/SQL-Data-Processing-templates-examples) . <br>\n",
    "Template is pretty much a \"reusable data job\". Anyone that writes a data job, can create their own template. See [Tempalate registry documentation](https://github.com/vmware/versatile-data-kit/blob/main/projects/vdk-core/src/vdk/api/plugin/plugin_input.py#L87)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac52510",
   "metadata": {},
   "source": [
    "### Build the ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804db6d",
   "metadata": {},
   "source": [
    "It's time to build the model. We focus on build-ml-model data job\n",
    "\n",
    "It has two steps. The first one would process the data so it is suitable for our ML model (linear regression)\n",
    "The second will build the actual ML Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21ab96",
   "metadata": {},
   "source": [
    "After we are done building the model let's try to run our job\n",
    "\n",
    "<font color='red'>**ATTENTION!**</font> Please change to the name of your data job if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0b1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdk run samples/build-ml-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacb70ac",
   "metadata": {},
   "source": [
    "### Build a Streamlit Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3512ca",
   "metadata": {},
   "source": [
    "Now that we have finished with the data job, let's use that hard-earned model to make a cool dashboard!\n",
    "\n",
    "When you run below command, you will get an output, but the kernel will be stuck. That's okay! Just open a new tab in your browser,\n",
    "copy the link of the MyBinder environment, delete everything after \"user/blah blah blah\" and paste \"/proxy/8501/\"\n",
    "So, something like this: \n",
    "```\n",
    "https://hub.gke2.mybinder.org/user/alexanderavramo-n-example-empty-zkd8q00p/proxy/8501/\n",
    "```\n",
    "\n",
    "The Streamlit dashboard will now show up!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b19f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! streamlit run samples/build_streamlit_dashboard.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a673786e",
   "metadata": {},
   "source": [
    "### Bonus: Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d25b9d",
   "metadata": {},
   "source": [
    "Since the analysis that we perform is on a regular basis, it makes sense to schedule our data job to run once per week. VDK allows the **automatic execution of data jobs by deploying them on a cloud server** which handles the regular execution as per schedule that the user defines. The deployment configurations are entered in the **\"config.ini\"** file that is required for deployment.  \n",
    "Let's open it up and examine the contents.\n",
    "\n",
    "In the first section [owner], we have specified the **team owning the data job**. In the second section [job] we defined the schedule of execution. It is in cron format (you can use [this website](https://crontab.guru/#*/20_*_*_*_*) to translate the cron schedule into a human-readable form). In this case, we want the schedule to run on the Monday of each week at 00:01am US time. Since VDK uses UTC time for schedule execution, the cron schedule indicates 05:01am UTC time. \n",
    "\n",
    "The config file could also include a [contacts] section which specifies whether any **notifications** are sent to specific emails upon job execution success, failure or deployment. In our case, we have left those empty.\n",
    "\n",
    "The last part of the config file contains the **VDK configuration settings** - the type of DB to which we will be ingesting, the DB location, schema and catalogue. \n",
    "\n",
    "For a full list and explanations of the configuration settings you could enter into the \"config.ini\" file of a data job, you can run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771474d6-3ab8-4a9d-94d6-45627bc0ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vdk config-help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a59f72-f774-4bfe-8eed-9ad0fd13a2eb",
   "metadata": {},
   "source": [
    "\n",
    "Let's now deploy the data job. We would need to install vdk-server.  \n",
    "\n",
    "The below commands are illusatrative and won't owrk in myBinder. See https://github.com/vmware/versatile-data-kit/wiki/Scheduling-a-Data-Job-for-automatic-execution for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54eb2d-7a7a-4259-b491-074ef9e38925",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!vdk deploy -n <job-name> -t team-awesome  -r \"Initial deploy\" -p /home/jovyan/<job-name>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030de733-89ce-4d27-aaba-eb6c89d6a575",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vdk deploy --show -n <job-name> -t team-awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1d0a4-dd3f-42fe-8339-6fdf3193575d",
   "metadata": {},
   "source": [
    "\n",
    "And if there's an issue revert: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627fbb2b-37cf-4a90-811a-ee2372951470",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "! vdk deploy --update --job-version <old-version> -n ingest-<unique-suffix> -t team_awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef347d",
   "metadata": {},
   "source": [
    "**Please share your feedback** : https://bit.ly/vdk-dsc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17f9b7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('vdk-oss-39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e00b8106e488857b585e3c18b5cbcd89c05e8998087dfa37a307556692e1465b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
